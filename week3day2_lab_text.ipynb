{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 3 Day 2. Lab. Text Processing \n",
    "\n",
    "In this lab I have generated some methods that will allow you to download posts from Reddit. It accepts a list of subreddits of arbitrary length, which are each processed independently and stored in a single `results` dictionary. The keys of the dictionary are the subreddits. Underneath each subreddit is a dictionary of sub-specific result objects, like \"vectorizer\" and \"top terms\".\n",
    "\n",
    "Please read through the code. You will need to add your username. The code is intentionally broken so you will need to add that before running this. Other than that you should not need to make any modifications to the cell with the `RedditScraper` class. \n",
    "\n",
    "In the cell below is some code to run these methods. At the top are some parameters that you should set. These are typically written in ALL CAPS. You should read the code to understand what they do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: \n",
    "    \n",
    "0. **Explore subreddits**. The below code uses 'ukpolitics', 'unitedkingdom', and\n",
    "   'uknews'. These were loosely motivated by an interest in whether uknews has become a\n",
    "   reactionary subreddit with generally conservative opinions. By comparing it to the\n",
    "   other two (which other work has suggested are generally quite similar), we might get\n",
    "   a sense of this from the top keywords. Try some other subs related to a topic where\n",
    "   you suspect there will be some interesting distinctions. Motivate the distinctions.\n",
    "   If you aren't sure about the subs, query an LLM (they are typically trained on a\n",
    "   _lot_ of Reddit data and will know good subs). So instead of trying for /r/men and\n",
    "   /r/women, if you ask about subs for gender-based interests, it might suggest\n",
    "   /r/TwoXChromosomes and /r/MensRights as interesting distinctions. \n",
    "\n",
    "> We are choosing to explore `r/TooAfraidToAsk`, `r/AskMen`, and `r/AskWomen`.\n",
    "\n",
    "1. **Understand the results data structure**. The `results` object returns the top 5\n",
    "   terms. How would you access more than 5 terms? Expand the results to see 10. Consider\n",
    "   what way is more general and flexible. How might you change the code so that there is\n",
    "   a `TOP_N = 10` which is then passed through the code so that the results dictionary\n",
    "   contains ten terms in the \"top_terms\" DataFrame rather than hard coding it in the\n",
    "   method below?\n",
    "\n",
    "> I added a `TOP_N` global variable that I can pass into `analyze_subreddit` to vary the\n",
    "> number of terms displayed.\n",
    "\n",
    "2. **Store results**. Every time we run the code we query Reddit again. How can we store\n",
    "   our data so that it is cached for another round? There are many approaches to this\n",
    "   and among your group you may discover everything from 'just save the json' to\n",
    "   'DataFrame and then export to feather' to some who would ambitiously use MongoDB.\n",
    "   Given this is a simple exercise for now, keep this step simple as you need it to be\n",
    "   while still usable enough if you want to add more data.\n",
    "\n",
    "> I implemented a `use_cache` option in the `get_subreddit_posts` method that reads from\n",
    "> and writes to JSON files in the `data/` directory if enabled.\n",
    "\n",
    "3. **Plot keywords over time**. Expand your results to anywhere from 250 upwards (I\n",
    "   would here cap at 500 max and think that the api might only return last 1000 but\n",
    "   untested). Determine the top keywords using TFIDF. Then plot the frequency of these\n",
    "   keywords over this time period for these results.\n",
    "\n",
    "> \n",
    "\n",
    "4. **Table the most common URLs for stories**. Triangulate these plots with a table summarising the top news outlets for this sub in this time period. Notice the starter code to process this from the posts data that has been stored in a large `submissions` dictionary. Note, this code does not turn all the `json` into a DataFrame, but extracts only the URL column and processes that. It also uses a _regular expression_ to separate out the top level domain, which may or may not be the most robust.  \n",
    "5. **Write a summary**. Solely for reflection at this point, write some intuitions that you discover with this exploration. \n",
    "\n",
    "## Caveats for the exercise: \n",
    "- Reddit might severely limit the number of posts you download using this scraper even with your name appropriately in the username, so be judicious with your exploration (hence exercise 2 _first_). \n",
    "- While you might not have extensive experience with Reddit, I can be confident that there are subreddits on most imaginable topics that can be found with little challenge. However, these subs will have vastly different numbers of subscribers and activity, so bear that in mind with any interpretation when tempted to generalise what is found _beyond_ Reddit (i.e. generalising from /r/republicans to Republicans in the US). \n",
    "- You may be tempted out of curiosity to expand your data collection. You will find that this will lead to a trade off if you do not further process your data. If you have 1000 rows for headlines and 3000 for words, that's a big matrix that has to be multiplied by vectors. At some point the size of the matrix will be unnecessary as well as slow. You may need to consider different parameters for `MIN_DOC_FREQ` to get a balance between a big matrix and a meaningful one. \n",
    "- These results have not been cleared for publication with CUREC, but only for use within classroom and for illustrative purposes. Please do not upload raw reddit data to your own GitHub archive nor seek to publish these results.  (Notice that I have pre-emtively edited the .gitignore to include a `data/` folder where you can store results without uploading them). Seek advice from research.fac@oii.ox.ac.uk for use for a comparable project should you wish to publish this work. If you wish to produce a blog post or other informal analysis, this should be presented in such a way that it is not misconstrued that the University has endorsed this work for publication. \n",
    "\n",
    "# Where we are headed with this exercise \n",
    "\n",
    "### Today: \n",
    "Collect reddit data, make it robust and explore TF-IDF results. \n",
    "\n",
    "### Week 3 Day 3. Friday: \n",
    "We use contine the use of the TF-IDF matrix and introduce cosine distance. We show how to plot it using t-SNE. This might sound abstract but the results will be fascinating as we see words plotted in coherent clusters that seem to reveal inductive patterns. \n",
    "\n",
    "Worksheets will be uploaded to this repo. \n",
    "\n",
    "### Week 4 Day 1. Monday: \n",
    "We will use two simple forms of classification, k-means and Naive Bayes Clustering. You might also be familiar with LDA or 'topic modelling'. We will not cover this as the technique deserves some care to understand its internals even if it is easy to run out of the box. But it is not far as an extension from where we end up. \n",
    "\n",
    "In the lab we will then compare classification results to results from the t-SNE and exploration of distance from Friday. \n",
    "\n",
    "### Week 4 Day 2. Wednesday: \n",
    "We will introduce the `networkx` and `community` package and show how to both construct a network from threaded comments and users of these comments. This will involve two types of graphs: DAGs and Bipartite graphs. \n",
    "\n",
    "In the lab you will have code that shows how to do this with the Reddit data in general. You will have to apply this to your specific case. \n",
    "\n",
    "### Week 4 Day 3. Friday: \n",
    "In the walkthrough we will see how to create 'embeddings' as abstractions even further than t-SNE but as a next-step up from cosine distance. In fact we will see how you can use cosine distance on embeddings which allows you to do these same steps not with words, but with entire sentences or whole paragraphs. We feature this on Friday and assume that your presentations will not need to use embeddings. \n",
    "\n",
    "In the afternoon you we will have the second set of group presentations: \n",
    "- Take a current event or coherent topic that could be collected from reddit data using the requests API (or more abstract packages such as `praw`, but not entire archive dumps like PushShift, only a limited subset). \n",
    "- Look at three or more subreddits who might speak to that topic. Determine which two subs are the most similar and why? Be sure to consider not only common word use. You may define similarity in creative ways so long as they can result in calculable differences without use of ML models, external APIs, or mass labelling of data. If you can download a lexicon, you are welcome to use scoring.\n",
    "- Motivate this topic deductively. Where possible try to draw upon any existing literature on the topic and not simply abductively from current events. Consider DIKW: Find ways to produce transferable _knowledge_ rather than merely _information_ from _data_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data from Reddit\n",
    "### Initialize scraper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditScraper:\n",
    "    def __init__(self, user_agent):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with a user agent string.\n",
    "        Example user agent: \"SDS_textanalysis/1.0 (by /u/your_username)\"\n",
    "        \"\"\"\n",
    "        self.headers = {'User-Agent': user_agent}\n",
    "        self.base_url = \"https://api.reddit.com\"\n",
    "        \n",
    "    def get_subreddit_posts(\n",
    "        self, subreddit: str, limit: int = 100, use_cache: bool = False\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"Collect posts from a subreddit with proper pagination and rate limiting.\n",
    "\n",
    "        Args:\n",
    "            subreddit (str): The name of the subreddit to collect from.\n",
    "            limit (int, optional): The limit to the number of posts fetched. Defaults to\n",
    "                100.\n",
    "            use_cache (bool, optional): Whether to read and write from JSON files in the\n",
    "            data/ dir as a cache. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            list[dict]: A list of structured data about posts from the subreddit, up to\n",
    "                length `limit`.\n",
    "        \"\"\"\n",
    "        posts = []\n",
    "        after = None\n",
    "\n",
    "        data_path = os.path.join(\"data\", f\"{subreddit}.json\")\n",
    "\n",
    "        # Check if cache exists\n",
    "        if use_cache and os.path.exists(data_path):\n",
    "            print(f\"Using cache at {data_path}.\")\n",
    "            return json.load(open(data_path))[:limit]\n",
    "        \n",
    "        while len(posts) < limit:\n",
    "            url = f\"{self.base_url}/r/{subreddit}/new\"\n",
    "            params = {\n",
    "                'limit': min(100, limit - len(posts)),\n",
    "                'after': after\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error accessing r/{subreddit}: {response.status_code}\")\n",
    "                break\n",
    "                \n",
    "            data = response.json()\n",
    "            new_posts = data['data']['children']\n",
    "            if not new_posts:\n",
    "                break\n",
    "                \n",
    "            posts.extend([post['data'] for post in new_posts])\n",
    "            after = data['data']['after']\n",
    "            \n",
    "            if not after:\n",
    "                break\n",
    "                \n",
    "            time.sleep(2)\n",
    "\n",
    "        # Save to cache if requested\n",
    "        if use_cache:\n",
    "            print(f\"Saving {len(posts[:limit])} posts to {data_path}.\")\n",
    "            with open(data_path, \"w\") as f:\n",
    "                json.dump(posts[:limit], f)\n",
    "\n",
    "        return posts[:limit]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def analyze_vocabulary(texts, min_freq=2):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary distribution in a corpus.\n",
    "    Returns word frequencies and vocabulary statistics.\n",
    "    \"\"\"\n",
    "    # Tokenize all texts\n",
    "    words = ' '.join(texts).split()\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Calculate vocabulary statistics\n",
    "    total_words = len(words)\n",
    "    unique_words = len(word_freq)\n",
    "    \n",
    "    # Create frequency distribution DataFrame\n",
    "    freq_df = pd.DataFrame(list(word_freq.items()), columns=['word', 'frequency'])\n",
    "    freq_df['percentage'] = freq_df['frequency'] / total_words * 100\n",
    "    freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "    \n",
    "    # Calculate cumulative coverage\n",
    "    freq_df['cumulative_percentage'] = freq_df['percentage'].cumsum()\n",
    "    \n",
    "    stats = {\n",
    "        'total_words': total_words,\n",
    "        'unique_words': unique_words,\n",
    "        'words_min_freq': sum(1 for freq in word_freq.values() if freq >= min_freq),\n",
    "        'coverage_top_1000': freq_df.iloc[:1000]['frequency'].sum() / total_words * 100 if len(freq_df) >= 1000 else 100\n",
    "    }\n",
    "    \n",
    "    return freq_df, stats\n",
    "\n",
    "def analyze_subreddit(\n",
    "    posts, max_terms: int = 1000, min_doc_freq: int = 2, top_n: int = 5\n",
    ") -> dict:\n",
    "    \"\"\"Analyze a single subreddit's posts independently.\n",
    "\n",
    "    Args:\n",
    "        posts (list[dict]): A list of structured data for the subreddit's posts.\n",
    "        max_terms (int, optional): The maximum number of terms to rank with the TF-IDF\n",
    "            vectorizer. Defaults to 1000.\n",
    "        min_doc_freq (int, optional): The minimum number of documents a term must appear\n",
    "            in to be included in the TF-IDF matrix. Defaults to 2.\n",
    "        top_n (int, optional): The number of top terms by aggregated TF-IDF score to\n",
    "            display. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Some structured data summarizing the subreddit's posts and key terms.\n",
    "    \"\"\"\n",
    "    # Use combined title and selftext\n",
    "    texts = posts[\"text\"]\n",
    "    \n",
    "    # Analyze vocabulary first\n",
    "    freq_df, vocab_stats = analyze_vocabulary(texts, min_freq=min_doc_freq)\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer for this subreddit\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        max_features=max_terms,\n",
    "        min_df=min_doc_freq\n",
    "    )\n",
    "    \n",
    "    # Compute TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Get average TF-IDF scores\n",
    "    mean_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "    \n",
    "    # Get top terms\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_terms = pd.DataFrame({\n",
    "        'term': feature_names,\n",
    "        'score': mean_tfidf\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'vocab_stats': vocab_stats,\n",
    "        'freq_distribution': freq_df,\n",
    "        'top_terms': top_terms.head(top_n),\n",
    "        'vectorizer': vectorizer,\n",
    "        'matrix_shape': tfidf_matrix.shape,\n",
    "        'matrix_sparsity': 100 * (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example subreddits\n",
    "subreddits = ['TooAfraidToAsk', 'AskMen', 'AskWomen']\n",
    "\n",
    "# Analysis parameters\n",
    "MAX_TERMS = 1000\n",
    "MIN_DOC_FREQ = 2\n",
    "LIMIT = 500\n",
    "TOP_N = 10  # The number of top terms to display from the results object\n",
    "USERNAME = os.getenv(\"REDDIT_USERNAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run scraper to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing r/TooAfraidToAsk...\n",
      "Using cache at data/TooAfraidToAsk.json.\n",
      "\n",
      "Vocabulary Statistics for r/TooAfraidToAsk:\n",
      "Total words: 52828\n",
      "Unique words: 5920\n",
      "Words appearing ≥2 times: 2923\n",
      "Coverage by top 1000 words: 84.01%\n",
      "Matrix shape: (500, 1000)\n",
      "Matrix sparsity: 97.29%\n",
      "\n",
      "Top 10 terms by TF-IDF score:\n",
      "       term     score\n",
      "440      im  0.044684\n",
      "500    like  0.044397\n",
      "648  people  0.039609\n",
      "238    dont  0.031056\n",
      "987   would  0.030627\n",
      "479    know  0.028720\n",
      "300    feel  0.026879\n",
      "346     get  0.024652\n",
      "461     ive  0.023110\n",
      "767     sex  0.022086\n",
      "\n",
      "Analyzing r/AskMen...\n",
      "Using cache at data/AskMen.json.\n",
      "\n",
      "Vocabulary Statistics for r/AskMen:\n",
      "Total words: 53148\n",
      "Unique words: 5349\n",
      "Words appearing ≥2 times: 2627\n",
      "Coverage by top 1000 words: 86.42%\n",
      "Matrix shape: (500, 1000)\n",
      "Matrix sparsity: 97.17%\n",
      "\n",
      "Top 10 terms by TF-IDF score:\n",
      "       term     score\n",
      "560     men  0.047653\n",
      "442      im  0.046468\n",
      "507    like  0.041020\n",
      "350     get  0.034013\n",
      "959   whats  0.031248\n",
      "986   would  0.029220\n",
      "481    know  0.029061\n",
      "233    dont  0.027567\n",
      "300    feel  0.027195\n",
      "706  really  0.022426\n",
      "\n",
      "Analyzing r/AskWomen...\n",
      "Using cache at data/AskWomen.json.\n",
      "\n",
      "Vocabulary Statistics for r/AskWomen:\n",
      "Total words: 13284\n",
      "Unique words: 2035\n",
      "Words appearing ≥2 times: 895\n",
      "Coverage by top 1000 words: 92.21%\n",
      "Matrix shape: (500, 708)\n",
      "Matrix sparsity: 98.74%\n",
      "\n",
      "Top 10 terms by TF-IDF score:\n",
      "        term     score\n",
      "681    whats  0.036233\n",
      "697    would  0.034933\n",
      "354     like  0.031382\n",
      "686    women  0.027963\n",
      "211     feel  0.024949\n",
      "352     life  0.024872\n",
      "250      get  0.021253\n",
      "452  partner  0.019694\n",
      "620    thing  0.018540\n",
      "337   ladies  0.018429\n"
     ]
    }
   ],
   "source": [
    "# Initialize scraper\n",
    "scraper = RedditScraper(\n",
    "user_agent=f\"SDS_textanalysis/1.0 (by /u/{USERNAME})\"\n",
    ")\n",
    "\n",
    "# Analyze each subreddit independently\n",
    "results = {}\n",
    "submissions = {}\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(f\"\\nAnalyzing r/{subreddit}...\")\n",
    "\n",
    "    # Collect posts\n",
    "    submissions[subreddit] = scraper.get_subreddit_posts(\n",
    "        subreddit, limit=LIMIT, use_cache=True\n",
    "    )\n",
    "\n",
    "    # Perform the text processing on submissions here rather than in the\n",
    "    # analyze_subreddit function; this way we get the processed text data saved in\n",
    "    # the submissions dict\n",
    "    submissions[subreddit] = [\n",
    "        {\n",
    "            **post,\n",
    "            \"text\": (\n",
    "                preprocess_text(post.get('title', '')) + ' ' +\n",
    "                preprocess_text(post.get('selftext', ''))\n",
    "            )\n",
    "        } for post in submissions[subreddit]\n",
    "    ]\n",
    "\n",
    "    # Analyze subreddit\n",
    "    results[subreddit] = analyze_subreddit(\n",
    "        submissions[subreddit],\n",
    "        max_terms=MAX_TERMS,   # Maximum number of terms to keep\n",
    "        min_doc_freq=MIN_DOC_FREQ,   # Term must appear in at least min_doc_freq documents\n",
    "        top_n=TOP_N   # Number of top terms to display\n",
    "    )\n",
    "\n",
    "    # Print results for this subreddit\n",
    "    print(f\"\\nVocabulary Statistics for r/{subreddit}:\")\n",
    "    print(f\"Total words: {results[subreddit]['vocab_stats']['total_words']}\")\n",
    "    print(f\"Unique words: {results[subreddit]['vocab_stats']['unique_words']}\")\n",
    "    print(f\"Words appearing ≥{MIN_DOC_FREQ} times: {results[subreddit]['vocab_stats']['words_min_freq']}\")\n",
    "    print(f\"Coverage by top {MAX_TERMS} words: {results[subreddit]['vocab_stats']['coverage_top_1000']:.2f}%\")\n",
    "    print(f\"Matrix shape: {results[subreddit]['matrix_shape']}\")\n",
    "    print(f\"Matrix sparsity: {results[subreddit]['matrix_sparsity']:.2f}%\")\n",
    "\n",
    "    print(f\"\\nTop {TOP_N} terms by TF-IDF score:\")\n",
    "    print(results[subreddit]['top_terms'][['term', 'score']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO consider removing analysis of links, given our choice of subreddits, they aren't\n",
    "# very interesting...\n",
    "# # Data Exploration:\n",
    "# example_sub = subreddits[2]\n",
    "\n",
    "# submissions[example_sub][0]\n",
    "\n",
    "# url_list = [post['url'] for post in submissions[example_sub]]\n",
    "# url_df = pd.DataFrame(url_list, columns=['url'])\n",
    "# url_df['domain'] = url_df['url'].str.extract(r'(https?://[^/]+)')\n",
    "\n",
    "# url_df['domain'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vocab_stats', 'freq_distribution', 'top_terms', 'vectorizer', 'matrix_shape', 'matrix_sparsity'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[subreddits[0]].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot keywords over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_richtext</th>\n",
       "      <th>...</th>\n",
       "      <th>permalink</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "      <th>text</th>\n",
       "      <th>author_cakeday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td></td>\n",
       "      <td>t2_7qoem3u9</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>if you’re equally racist towards everyone, are...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gkdou4/if_youre_eq...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730832e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>if youre equally racist towards everyone are y...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>My niece  just turned 17,  her ex boyfriend ke...</td>\n",
       "      <td>t2_j96sx</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Phone harassment, what to do?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gk85ya/phone_haras...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730818e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>phone harassment what to do my niece just turn...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>So as the title says. I work in a scummy gas s...</td>\n",
       "      <td>t2_3xp1zhoz</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>I called emailed a health inspector on my job ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gkdlns/i_called_em...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730831e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>i called emailed a health inspector on my job ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>So I was watching \"The Substance\" and this que...</td>\n",
       "      <td>t2_3gjosbej</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>If we have sex with the clone of ourselves in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gkd6dv/if_we_have_...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730830e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>if we have sex with the clone of ourselves in ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>I know the reason in America is partly because...</td>\n",
       "      <td>t2_swemzibz4</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Why are so many people of African descent Chri...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gkcd7u/why_are_so_...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730828e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>why are so many people of african descent chri...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>Women of reddit, is it a turn off if a guy loo...</td>\n",
       "      <td>t2_d9wno</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Women of reddit, is it a turn off if a guy loo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gf9bod/women_of_re...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730246e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>women of reddit is it a turn off if a guy look...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>I am American but my dad has always moved us i...</td>\n",
       "      <td>t2_e9vfoygjk</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>what makes the usa education system so bad?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gf8kga/what_makes_...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730244e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>what makes the usa education system so bad i a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>I recently finished a pretty strict diet and t...</td>\n",
       "      <td>t2_130duwa7gt</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Why does eating \"normally\" after a diet make m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gf8k85/why_does_ea...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730244e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>why does eating normally after a diet make me ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>To preface this, I don’t take any sorts or rec...</td>\n",
       "      <td>t2_4v8thi69</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>How far up your bum does a smelly substance, l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gf81fm/how_far_up_...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730243e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>how far up your bum does a smelly substance li...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>None</td>\n",
       "      <td>TooAfraidToAsk</td>\n",
       "      <td>I noticed in the sub roast me people having ba...</td>\n",
       "      <td>t2_3ezesy6i</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Why do some people ask to be roasted?</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>/r/TooAfraidToAsk/comments/1gf7go2/why_do_some...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.reddit.com/r/TooAfraidToAsk/commen...</td>\n",
       "      <td>1827819</td>\n",
       "      <td>1.730241e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>why do some people ask to be roasted i noticed...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    approved_at_utc       subreddit  \\\n",
       "0              None  TooAfraidToAsk   \n",
       "1              None  TooAfraidToAsk   \n",
       "2              None  TooAfraidToAsk   \n",
       "3              None  TooAfraidToAsk   \n",
       "4              None  TooAfraidToAsk   \n",
       "..              ...             ...   \n",
       "495            None  TooAfraidToAsk   \n",
       "496            None  TooAfraidToAsk   \n",
       "497            None  TooAfraidToAsk   \n",
       "498            None  TooAfraidToAsk   \n",
       "499            None  TooAfraidToAsk   \n",
       "\n",
       "                                              selftext author_fullname  saved  \\\n",
       "0                                                          t2_7qoem3u9  False   \n",
       "1    My niece  just turned 17,  her ex boyfriend ke...        t2_j96sx  False   \n",
       "2    So as the title says. I work in a scummy gas s...     t2_3xp1zhoz  False   \n",
       "3    So I was watching \"The Substance\" and this que...     t2_3gjosbej  False   \n",
       "4    I know the reason in America is partly because...    t2_swemzibz4  False   \n",
       "..                                                 ...             ...    ...   \n",
       "495  Women of reddit, is it a turn off if a guy loo...        t2_d9wno  False   \n",
       "496  I am American but my dad has always moved us i...    t2_e9vfoygjk  False   \n",
       "497  I recently finished a pretty strict diet and t...   t2_130duwa7gt  False   \n",
       "498  To preface this, I don’t take any sorts or rec...     t2_4v8thi69  False   \n",
       "499  I noticed in the sub roast me people having ba...     t2_3ezesy6i  False   \n",
       "\n",
       "    mod_reason_title  gilded  clicked  \\\n",
       "0               None       0    False   \n",
       "1               None       0    False   \n",
       "2               None       0    False   \n",
       "3               None       0    False   \n",
       "4               None       0    False   \n",
       "..               ...     ...      ...   \n",
       "495             None       0    False   \n",
       "496             None       0    False   \n",
       "497             None       0    False   \n",
       "498             None       0    False   \n",
       "499             None       0    False   \n",
       "\n",
       "                                                 title link_flair_richtext  \\\n",
       "0    if you’re equally racist towards everyone, are...                  []   \n",
       "1                        Phone harassment, what to do?                  []   \n",
       "2    I called emailed a health inspector on my job ...                  []   \n",
       "3    If we have sex with the clone of ourselves in ...                  []   \n",
       "4    Why are so many people of African descent Chri...                  []   \n",
       "..                                                 ...                 ...   \n",
       "495  Women of reddit, is it a turn off if a guy loo...                  []   \n",
       "496        what makes the usa education system so bad?                  []   \n",
       "497  Why does eating \"normally\" after a diet make m...                  []   \n",
       "498  How far up your bum does a smelly substance, l...                  []   \n",
       "499             Why do some people ask to be roasted?                   []   \n",
       "\n",
       "     ...                                          permalink  stickied  \\\n",
       "0    ...  /r/TooAfraidToAsk/comments/1gkdou4/if_youre_eq...     False   \n",
       "1    ...  /r/TooAfraidToAsk/comments/1gk85ya/phone_haras...     False   \n",
       "2    ...  /r/TooAfraidToAsk/comments/1gkdlns/i_called_em...     False   \n",
       "3    ...  /r/TooAfraidToAsk/comments/1gkd6dv/if_we_have_...     False   \n",
       "4    ...  /r/TooAfraidToAsk/comments/1gkcd7u/why_are_so_...     False   \n",
       "..   ...                                                ...       ...   \n",
       "495  ...  /r/TooAfraidToAsk/comments/1gf9bod/women_of_re...     False   \n",
       "496  ...  /r/TooAfraidToAsk/comments/1gf8kga/what_makes_...     False   \n",
       "497  ...  /r/TooAfraidToAsk/comments/1gf8k85/why_does_ea...     False   \n",
       "498  ...  /r/TooAfraidToAsk/comments/1gf81fm/how_far_up_...     False   \n",
       "499  ...  /r/TooAfraidToAsk/comments/1gf7go2/why_do_some...     False   \n",
       "\n",
       "                                                   url subreddit_subscribers  \\\n",
       "0    https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "1    https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "2    https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "3    https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "4    https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "..                                                 ...                   ...   \n",
       "495  https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "496  https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "497  https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "498  https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "499  https://www.reddit.com/r/TooAfraidToAsk/commen...               1827819   \n",
       "\n",
       "      created_utc num_crossposts  media is_video  \\\n",
       "0    1.730832e+09              0   None    False   \n",
       "1    1.730818e+09              0   None    False   \n",
       "2    1.730831e+09              0   None    False   \n",
       "3    1.730830e+09              0   None    False   \n",
       "4    1.730828e+09              0   None    False   \n",
       "..            ...            ...    ...      ...   \n",
       "495  1.730246e+09              0   None    False   \n",
       "496  1.730244e+09              0   None    False   \n",
       "497  1.730244e+09              0   None    False   \n",
       "498  1.730243e+09              0   None    False   \n",
       "499  1.730241e+09              0   None    False   \n",
       "\n",
       "                                                  text author_cakeday  \n",
       "0    if youre equally racist towards everyone are y...            NaN  \n",
       "1    phone harassment what to do my niece just turn...            NaN  \n",
       "2    i called emailed a health inspector on my job ...            NaN  \n",
       "3    if we have sex with the clone of ourselves in ...            NaN  \n",
       "4    why are so many people of african descent chri...            NaN  \n",
       "..                                                 ...            ...  \n",
       "495  women of reddit is it a turn off if a guy look...            NaN  \n",
       "496  what makes the usa education system so bad i a...            NaN  \n",
       "497  why does eating normally after a diet make me ...            NaN  \n",
       "498  how far up your bum does a smelly substance li...            NaN  \n",
       "499  why do some people ask to be roasted i noticed...            NaN  \n",
       "\n",
       "[500 rows x 105 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(submissions[subreddits[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['im' 'like' 'people' 'dont' 'would' 'know' 'feel' 'get' 'ive' 'sex']\n",
      "['men' 'im' 'like' 'get' 'whats' 'would' 'know' 'dont' 'feel' 'really']\n",
      "['whats' 'would' 'like' 'women' 'feel' 'life' 'get' 'partner' 'thing'\n",
      " 'ladies']\n"
     ]
    }
   ],
   "source": [
    "posts_dfs: dict[str, pd.DataFrame] = {}\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    top_terms = results[subreddit][\"top_terms\"][\"term\"].values[:TOP_N]\n",
    "\n",
    "    posts_dfs[subreddit] = pd.DataFrame(submissions[subreddit])\n",
    "\n",
    "    print(top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Declaration: \n",
    "\n",
    "Claude Sonnet 3.5 New produced much of the reddit code. I was surprised at how similar it was to my past code (knowing it was trained on GitHub I have to wonder). Several tweaks had to be made such as removing a main() function, altering the results object, altering some NLTK packages, adding the submissions dictionary and the submissions dictionary code. I kept in the `preprocess_text()` function as is, but you are encouraged to consider alternative forms of pre-processing from the walkthrough including the use of standard tokenizers, lemmatisation, and stop-words. It also used anodyne programming subreddits which I changed and I reduced the limit to 50 which is just enough to get two queries illustrating that you can get N queries through this approach. \n",
    "\n",
    "The URL code was written in VS code with co-pilot. Notably the autocomplete did an excellent job of anticipating steps with minimal prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
