{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "## Week 3 Day 2. Lab. Text Processing \n",
    "\n",
    "In this lab I have generated some methods that will allow you to download posts from Reddit. It accepts a list of subreddits of arbitrary length, which are each processed independently and stored in a single `results` dictionary. The keys of the dictionary are the subreddits. Underneath each subreddit is a dictionary of sub-specific result objects, like \"vectorizer\" and \"top terms\".\n",
    "\n",
    "Please read through the code. You will need to add your username. The code is intentionally broken so you will need to add that before running this. Other than that you should not need to make any modifications to the cell with the `RedditScraper` class. \n",
    "\n",
    "In the cell below is some code to run these methods. At the top are some parameters that you should set. These are typically written in ALL CAPS. You should read the code to understand what they do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: \n",
    "    \n",
    "0. **Explore subreddits**. The below code uses 'ukpolitics', 'unitedkingdom', and 'uknews'. These were loosely motivated by an interest in whether uknews has become a reactionary subreddit with generally conservative opinions. By comparing it to the other two (which other work has suggested are generally quite similar), we might get a sense of this from the top keywords. Try some other subs related to a topic where you suspect there will be some interesting distinctions. Motivate the distinctions. If you aren't sure about the subs, query an LLM (they are typically trained on a _lot_ of Reddit data and will know good subs). So instead of trying for /r/men and /r/women, if you ask about subs for gender-based interests, it might suggest /r/TwoXChromosomes and /r/MensRights as interesting distinctions. \n",
    "1. **Understand the results data structure**. The `results` object returns the top 5\n",
    "   terms. How would you access more than 5 terms? Expand the results to see 10. Consider\n",
    "   what way is more general and flexible. How might you change the code so that there is\n",
    "   a `TOP_N = 10` which is then passed through the code so that the results dictionary\n",
    "   contains ten terms in the \"top_terms\" DataFrame rather than hard coding it in the\n",
    "   method below?\n",
    "\n",
    "> I added a `TOP_N` global variable and pass it into `analyze_subreddits` as an optional\n",
    "> keyword argument.\n",
    "\n",
    "2. **Store results**. Every time we run the code we query Reddit again. How can we store\n",
    "   our data so that it is cached for another round? There are many approaches to this\n",
    "   and among your group you may discover everything from 'just save the json' to\n",
    "   'DataFrame and then export to feather' to some who would ambitiously use MongoDB.\n",
    "   Given this is a simple exercise for now, keep this step simple as you need it to be\n",
    "   while still usable enough if you want to add more data.\n",
    "\n",
    "> I created a `get_subreddit_data` function that writes and loads JSON from the `data/`\n",
    "> dir.\n",
    "\n",
    "3. **Plot keywords over time**. Expand your results to anywhere from 250 upwards (I would here cap at 500 max and think that the api might only return last 1000 but untested). Determine the top keywords using TFIDF. Then plot the frequency of these keywords over this time period for these results.   \n",
    "4. **Table the most common URLs for stories**. Triangulate these plots with a table summarising the top news outlets for this sub in this time period. Notice the starter code to process this from the posts data that has been stored in a large `submissions` dictionary. Note, this code does not turn all the `json` into a DataFrame, but extracts only the URL column and processes that. It also uses a _regular expression_ to separate out the top level domain, which may or may not be the most robust.  \n",
    "5. **Write a summary**. Solely for reflection at this point, write some intuitions that you discover with this exploration. \n",
    "\n",
    "## Caveats for the exercise: \n",
    "- Reddit might severely limit the number of posts you download using this scraper even with your name appropriately in the username, so be judicious with your exploration (hence exercise 2 _first_). \n",
    "- While you might not have extensive experience with Reddit, I can be confident that there are subreddits on most imaginable topics that can be found with little challenge. However, these subs will have vastly different numbers of subscribers and activity, so bear that in mind with any interpretation when tempted to generalise what is found _beyond_ Reddit (i.e. generalising from /r/republicans to Republicans in the US). \n",
    "- You may be tempted out of curiosity to expand your data collection. You will find that this will lead to a trade off if you do not further process your data. If you have 1000 rows for headlines and 3000 for words, that's a big matrix that has to be multiplied by vectors. At some point the size of the matrix will be unnecessary as well as slow. You may need to consider different parameters for `MIN_DOC_FREQ` to get a balance between a big matrix and a meaningful one. \n",
    "- These results have not been cleared for publication with CUREC, but only for use within classroom and for illustrative purposes. Please do not upload raw reddit data to your own GitHub archive nor seek to publish these results.  (Notice that I have pre-emtively edited the .gitignore to include a `data/` folder where you can store results without uploading them). Seek advice from research.fac@oii.ox.ac.uk for use for a comparable project should you wish to publish this work. If you wish to produce a blog post or other informal analysis, this should be presented in such a way that it is not misconstrued that the University has endorsed this work for publication. \n",
    "\n",
    "# Where we are headed with this exercise \n",
    "\n",
    "### Today: \n",
    "Collect reddit data, make it robust and explore TF-IDF results. \n",
    "\n",
    "### Week 3 Day 3. Friday: \n",
    "We use contine the use of the TF-IDF matrix and introduce cosine distance. We show how to plot it using t-SNE. This might sound abstract but the results will be fascinating as we see words plotted in coherent clusters that seem to reveal inductive patterns. \n",
    "\n",
    "Worksheets will be uploaded to this repo. \n",
    "\n",
    "### Week 4 Day 1. Monday: \n",
    "We will use two simple forms of classification, k-means and Naive Bayes Clustering. You might also be familiar with LDA or 'topic modelling'. We will not cover this as the technique deserves some care to understand its internals even if it is easy to run out of the box. But it is not far as an extension from where we end up. \n",
    "\n",
    "In the lab we will then compare classification results to results from the t-SNE and exploration of distance from Friday. \n",
    "\n",
    "### Week 4 Day 2. Wednesday: \n",
    "We will introduce the `networkx` and `community` package and show how to both construct a network from threaded comments and users of these comments. This will involve two types of graphs: DAGs and Bipartite graphs. \n",
    "\n",
    "In the lab you will have code that shows how to do this with the Reddit data in general. You will have to apply this to your specific case. \n",
    "\n",
    "### Week 4 Day 3. Friday: \n",
    "In the walkthrough we will see how to create 'embeddings' as abstractions even further than t-SNE but as a next-step up from cosine distance. In fact we will see how you can use cosine distance on embeddings which allows you to do these same steps not with words, but with entire sentences or whole paragraphs. We feature this on Friday and assume that your presentations will not need to use embeddings. \n",
    "\n",
    "In the afternoon you we will have the second set of group presentations: \n",
    "- Take a current event or coherent topic that could be collected from reddit data using the requests API (or more abstract packages such as `praw`, but not entire archive dumps like PushShift, only a limited subset). \n",
    "- Look at three or more subreddits who might speak to that topic. Determine which two subs are the most similar and why? Be sure to consider not only common word use. You may define similarity in creative ways so long as they can result in calculable differences without use of ML models, external APIs, or mass labelling of data. If you can download a lexicon, you are welcome to use scoring.\n",
    "- Motivate this topic deductively. Where possible try to draw upon any existing literature on the topic and not simply abductively from current events. Consider DIKW: Find ways to produce transferable _knowledge_ rather than merely _information_ from _data_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditScraper:\n",
    "    def __init__(self, user_agent):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with a user agent string.\n",
    "        Example user agent: \"SDS_textanalysis/1.0 (by /u/your_username)\"\n",
    "        \"\"\"\n",
    "        self.headers = {\"User-Agent\": user_agent}\n",
    "        self.base_url = \"https://api.reddit.com\"\n",
    "\n",
    "    def get_subreddit_posts(self, subreddit, limit=100):\n",
    "        \"\"\"\n",
    "        Collect posts from a subreddit with proper pagination and rate limiting.\n",
    "        \"\"\"\n",
    "        posts = []\n",
    "        after = None\n",
    "\n",
    "        while len(posts) < limit:\n",
    "            url = f\"{self.base_url}/r/{subreddit}/new\"\n",
    "            params = {\"limit\": min(100, limit - len(posts)), \"after\": after}\n",
    "\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Error accessing r/{subreddit}: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            data = response.json()\n",
    "            new_posts = data[\"data\"][\"children\"]\n",
    "            if not new_posts:\n",
    "                break\n",
    "\n",
    "            posts.extend([post[\"data\"] for post in new_posts])\n",
    "            after = data[\"data\"][\"after\"]\n",
    "\n",
    "            if not after:\n",
    "                break\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "        return posts[:limit]\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def analyze_vocabulary(texts, min_freq=2):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary distribution in a corpus.\n",
    "    Returns word frequencies and vocabulary statistics.\n",
    "    \"\"\"\n",
    "    # Tokenize all texts\n",
    "    words = texts.split(\" \")\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # Calculate vocabulary statistics\n",
    "    total_words = len(words)\n",
    "    unique_words = len(word_freq)\n",
    "\n",
    "    # Create frequency distribution DataFrame\n",
    "    freq_df = pd.DataFrame(list(word_freq.items()), columns=[\"word\", \"frequency\"])\n",
    "    freq_df[\"percentage\"] = freq_df[\"frequency\"] / total_words * 100\n",
    "    freq_df = freq_df.sort_values(\"frequency\", ascending=False)\n",
    "\n",
    "    # Calculate cumulative coverage\n",
    "    freq_df[\"cumulative_percentage\"] = freq_df[\"percentage\"].cumsum()\n",
    "\n",
    "    stats = {\n",
    "        \"total_words\": total_words,\n",
    "        \"unique_words\": unique_words,\n",
    "        \"words_min_freq\": sum(1 for freq in word_freq.values() if freq >= min_freq),\n",
    "        \"coverage_top_1000\": (\n",
    "            freq_df.iloc[:1000][\"frequency\"].sum() / total_words * 100\n",
    "            if len(freq_df) >= 1000\n",
    "            else 100\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return freq_df, stats\n",
    "\n",
    "\n",
    "def analyze_subreddits(\n",
    "    subreddits: dict[str, list],\n",
    "    max_terms=1000,\n",
    "    min_doc_freq=2,\n",
    "    top_n: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze a collection of subreddits using TF-IDF.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing collection of {len(subreddits)} subreddits.\")\n",
    "\n",
    "    subreddit_docs: dict[str, str] = {}\n",
    "    for subreddit, posts in subreddits.items():\n",
    "        # Each subreddit document is all posts joined together with newline delimiting\n",
    "        subreddit_docs[subreddit] = \"\\n\".join(\n",
    "            [\n",
    "                preprocess_text(post.get(\"title\", \"\"))\n",
    "                + \" \"\n",
    "                + preprocess_text(post.get(\"selftext\", \"\"))\n",
    "                for post in posts\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Analyze vocabulary\n",
    "        freq_df, vocab_stats = analyze_vocabulary(\n",
    "            subreddit_docs[subreddit], min_freq=min_doc_freq\n",
    "        )\n",
    "\n",
    "    # Initialize TF-IDF vectorizer for subreddit corpus\n",
    "    stop_words = list(set(stopwords.words(\"english\")))\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words, max_features=max_terms, min_df=min_doc_freq\n",
    "    )\n",
    "\n",
    "    # Compute TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform(subreddit_docs.values())\n",
    "\n",
    "    print(f\"Fit TF-IDF. Matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Iterate over each document\n",
    "    for i, row in enumerate(tfidf_matrix):\n",
    "        # Convert the row to a dense array, then get the top_n indices by score\n",
    "        row_data = row.toarray().flatten()\n",
    "        top_indices = row_data.argsort()[-top_n:][::-1]  # top_n indices, sorted by score\n",
    "\n",
    "        # Print the top terms with their scores\n",
    "        print(f\"Document ({list(subreddits.keys())[i]}) top terms:\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"  {terms[idx]}: {row_data[idx]}\")\n",
    "\n",
    "    return vectorizer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example subreddits\n",
    "subreddits = [\n",
    "    \"Futurology\",\n",
    "    \"artificial\",\n",
    "    \"LateStageCapitalism\",\n",
    "    \"Antiwork\",\n",
    "    \"BigTech\",\n",
    "    \"DeGoogle\",\n",
    "]\n",
    "\n",
    "# Analysis parameters\n",
    "MAX_TERMS = 1000\n",
    "MIN_DOC_FREQ = 1\n",
    "LIMIT = 500\n",
    "USERNAME = os.getenv(\"REDDIT_USERNAME\")  # Replace with your Reddit username\n",
    "TOP_N = 3\n",
    "\n",
    "# Initialize scraper\n",
    "scraper = RedditScraper(user_agent=f\"SDS_textanalysis/1.0 (by /u/{USERNAME})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze collection of subreddits together\n",
    "# The whole collection constitutes the \"corpus\", each subreddit is a \"document\"\n",
    "def get_subreddit_data(\n",
    "    scraper: RedditScraper, subreddits: list[str], limit: int = 500, force: bool = False\n",
    ") -> dict[str, list]:\n",
    "    for subreddit in subreddits:\n",
    "        file = os.path.join(os.getcwd(), \"data\", subreddit + \".json\")\n",
    "        if not os.path.exists(file) or force:\n",
    "            print(f\"Collecting posts for r/{subreddit}...\")\n",
    "            subreddit_data = scraper.get_subreddit_posts(subreddit, limit=limit)\n",
    "            json.dump(subreddit_data, open(file, \"w\"))\n",
    "\n",
    "    ret = {}\n",
    "    for subreddit in subreddits:\n",
    "        file = os.path.join(os.getcwd(), \"data\", subreddit + \".json\")\n",
    "        ret[subreddit] = json.load(open(file, \"r\"))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting posts for r/Futurology...\n",
      "Collecting posts for r/artificial...\n",
      "Collecting posts for r/LateStageCapitalism...\n",
      "Collecting posts for r/Antiwork...\n",
      "Collecting posts for r/BigTech...\n",
      "Collecting posts for r/DeGoogle...\n"
     ]
    }
   ],
   "source": [
    "submissions = get_subreddit_data(scraper, subreddits, limit=LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing collection of 6 subreddits.\n",
      "Fit TF-IDF. Matrix shape: (6, 1000)\n",
      "Document (Futurology) top terms:\n",
      "  ai: 0.3817128613589031\n",
      "  could: 0.21004305598585143\n",
      "  would: 0.21004305598585143\n",
      "Document (artificial) top terms:\n",
      "  ai: 0.7857689335179799\n",
      "  oneminute: 0.13253439418595744\n",
      "  like: 0.11766386081397444\n",
      "Document (LateStageCapitalism) top terms:\n",
      "  us: 0.2680487778406885\n",
      "  people: 0.23856341227821276\n",
      "  claudia: 0.22946313697857904\n",
      "Document (Antiwork) top terms:\n",
      "  work: 0.3369234744251391\n",
      "  job: 0.28770490578401653\n",
      "  im: 0.28546769811851097\n",
      "Document (BigTech) top terms:\n",
      "  apple: 0.35197323145420634\n",
      "  google: 0.3322386778748386\n",
      "  media: 0.2633599275837135\n",
      "Document (DeGoogle) top terms:\n",
      "  google: 0.48134015103227434\n",
      "  apps: 0.2104799084056425\n",
      "  im: 0.20702802194936532\n"
     ]
    }
   ],
   "source": [
    "vectorizer, matrix = analyze_subreddits(submissions, max_terms=MAX_TERMS, min_doc_freq=MIN_DOC_FREQ, top_n=TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "https://www.reddit.com              432\n",
       "https://i.redd.it                    25\n",
       "https://www.androidauthority.com      5\n",
       "https://www.theverge.com              4\n",
       "https://tuta.com                      3\n",
       "https://cybernews.com                 2\n",
       "https://github.com                    2\n",
       "https://www.androidpolice.com         2\n",
       "https://www.dailydot.com              1\n",
       "https://proton.me                     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration:\n",
    "example_subreddit = \"DeGoogle\"\n",
    "\n",
    "submissions[example_subreddit][0] # Example post from the unitedkingdom subreddit\n",
    "\n",
    "url_list = [post['url'] for post in submissions[example_subreddit]]\n",
    "url_df = pd.DataFrame(url_list, columns=['url'])\n",
    "url_df['domain'] = url_df['url'].str.extract(r'(https?://[^/]+)')\n",
    "\n",
    "url_df['domain'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Declaration: \n",
    "\n",
    "Claude Sonnet 3.5 New produced much of the reddit code. I was surprised at how similar it was to my past code (knowing it was trained on GitHub I have to wonder). Several tweaks had to be made such as removing a main() function, altering the results object, altering some NLTK packages, adding the submissions dictionary and the submissions dictionary code. I kept in the `preprocess_text()` function as is, but you are encouraged to consider alternative forms of pre-processing from the walkthrough including the use of standard tokenizers, lemmatisation, and stop-words. It also used anodyne programming subreddits which I changed and I reduced the limit to 50 which is just enough to get two queries illustrating that you can get N queries through this approach. \n",
    "\n",
    "The URL code was written in VS code with co-pilot. Notably the autocomplete did an excellent job of anticipating steps with minimal prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
